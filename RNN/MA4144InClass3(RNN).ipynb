{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda0e25f-476a-43c0-b3e7-f96a707be566",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>Recurrent Neural Networks</center>\n",
    "## <center>Inclass Project 3 - MA4144</center>\n",
    "### <center>Ishara Dilshan Rajarathna - 200500L</center>\n",
    "\n",
    "This project contains multiple tasks to be completed, some require written answers. Open a markdown cell below the respective question that require written answers and provide (type) your answers. Questions that required written answers are given in blue fonts. Almost all written questions are open ended, they do not have a correct or wrong answer. You are free to give your opinions, but please provide related answers within the context.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload both this ipynb file and the PDF on moodle**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930b71-f7aa-4eb8-a078-70fc89a1ac16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Outline of the project\n",
    "\n",
    "The aim of the project is to build a RNN model to suggest autocompletion of half typed words. You may have seen this in many day today applications; typing an email, a text message etc. For example, suppose you type in the four letter \"univ\", the application may suggest you to autocomplete it by \"university\".\n",
    "\n",
    "![Autocomplete](https://d33v4339jhl8k0.cloudfront.net/docs/assets/5c12e83004286304a71d5b72/images/66d0cb106eb51e63b8f9fbc6/file-gBQe016VYt.gif)\n",
    "\n",
    "We will train a RNN to suggest possible autocompletes given $3$ - $4$ starting letters. That is if we input a string \"univ\" hopefully we expect to see an output like \"university\", \"universal\" etc.\n",
    "\n",
    "For this we will use a text file (wordlist.txt) containing 10,000 common English words (you'll find the file on the moodle link). The list of words will be the \"**vocabulary**\" for our model.\n",
    "\n",
    "We will use the Python **torch library** to implement our autocomplete model. \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db6bc0-f7e0-473d-a172-e6579deea2ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the below cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76fdc286-3211-4a8f-9802-29d28a324bea",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622b61b-dba8-47bb-8e07-92b77e78f4fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 1: Preparing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "555a82e5-e56c-4075-a2a2-071633cd4d4c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "WORD_SIZE = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f44ef-91d0-4d0e-afb5-a66240c9e1d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Instructions"
   },
   "source": [
    "**Q1.** In the following cell provide code to load the text file (each word is in a newline), then extract the words (in lowercase) into a list.\n",
    "\n",
    "For practical reasons of training the model we will only use words that are longer that $3$ letters and that have a maximum length of WORD_SIZE (this will be a constant we set at the beginning - you can change this and experiment with different WORD_SIZEs). As seen above it is set to $13$.\n",
    "\n",
    "So out of the extracted list of words filter out those words that match our criteria on word length.\n",
    "\n",
    "To train our model it is convenient to have words/strings of equal length. We will choose to convert every word to length of WORD_SIZE, by adding underscores to the end of the word if it is initially shorter than WORD_SIZE. For example, we will convert the word \"university\" (word length 10) into \"university___\" (wordlength 13). In your code include this conversion as well.\n",
    "\n",
    "Store the processed WORD_SIZE lengthed strings in a list called vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3219551c-a298-424a-a491-2d7f65a4ad6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def load_vocab(file_path):\n",
    "    vocab = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip().lower()\n",
    "            \n",
    "            # Filter out words shorter than or equal to 3 or longer than WORD_SIZE\n",
    "            if 3 < len(word) <= WORD_SIZE:\n",
    "                padded_word = word.ljust(WORD_SIZE, '_')\n",
    "                vocab.append(padded_word)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab('wordlist.txt')\n",
    "#print(f\"Vocabulary size: {len(vocab)}\")\n",
    "#print(f\"First 10 words: {vocab[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3a6fe-c0a7-4808-aa1a-4c3ad6db6de3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font color='blue'>In the above explanation it was mentioned \"for practical reasons of training the model we will only use words that are longer that $3$ letters and that have a certain maximum length\". In your opinion what could be those practical? Will hit help to build a better model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf86cdd-96b2-40eb-8230-c3f04e93cfc4",
   "metadata": {
    "deletable": false,
    "id": "Ans1"
   },
   "source": [
    "Very short words—such as prepositions, articles, and conjunctions—often carry limited semantic information and can introduce noise rather than useful patterns. On the other hand, excessively long words are typically rare, which makes them less useful for learning generalized representations and can unnecessarily increase the model's complexity and memory usage. By discarding these extremes, we simplify the dataset, reduce the vocabulary size, and help the model focus on learning from more informative and representative word structures. This ultimately leads to more efficient training and potentially better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b2e6d-f771-4782-8b21-73c121565faa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2** To input words into the model, we will need to convert each letter/character into a number. as we have seen above, the only characters in our list vocab will be the underscore and lowercase english letters. so we will convert these $27$ characters into numbers as follows: underscore -> $0$, 'a' -> $1$, 'b' -> $2$, $\\cdots$, 'z' -> $26$. In the following cell,\n",
    "\n",
    "(i) Implement a method called char_to_num, that takes in a valid character and outputs its numerical assignment.\n",
    "\n",
    "(ii) Implement a method called num_to_char, that takes in a valid number from $0$ to $26$ and outputs the corresponding character.\n",
    "\n",
    "(iii) Implement a method called word_to_numlist, that takes in a word from our vocabulary and outputs a (torch) tensor of numbers that corresponds to each character in the word in that order. For example: the word \"united_______\" will be converted to tensor([21, 14,  9, 20,  5,  4,  0,  0,  0,  0,  0,  0,  0]). You are encouraged to use your char_to_num method for this.\n",
    "\n",
    "(iv) Implement a method called numlist_to_word, that does the opposite of the above described word_to_numlist, given a tensor of numbers from $0$ to $26$, outputs the corresponding word. You are encouraged to use your  num_to_char method for this.\n",
    "\n",
    "Note: As mentioned since we are using the torch library we will be using tensors instead of the usual python lists or numpy arrays. Tensors are the list equivalent in torch. Torch models only accept tensors as input and they output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "054a4ab4-5883-4948-adc5-eb8916b6234d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def char_to_num(char):\n",
    "\n",
    "    if char == '_':\n",
    "     num = 0\n",
    "    elif char.isalpha() and char.islower():\n",
    "        num = ord(char) - ord('a') + 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid character\")\n",
    "\n",
    "    return(num)\n",
    "\n",
    "def num_to_char(num):\n",
    "\n",
    "    if num == 0:\n",
    "        char = '_'\n",
    "    elif 0 < num <= 26:\n",
    "        char =  chr(num + ord('a') - 1)\n",
    "    else: \n",
    "        raise ValueError(f\"out of range number\")\n",
    "\n",
    "    return(char)\n",
    "\n",
    "def word_to_numlist(word):\n",
    "\n",
    "    numlist = torch.tensor([char_to_num(i) for i in word])\n",
    "    \n",
    "    return(numlist)\n",
    "\n",
    "def numlist_to_word(numlist):\n",
    "\n",
    "    word = ''.join(num_to_char(i.item()) for i in numlist)\n",
    "        \n",
    "    return(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d1936-fadb-4ddb-9027-3a75960aa6b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font color='blue'>We convert letter into just numbers based on their aphabetical order, I claim that it is a very bad way to encode data such as letters to be fed into learning models, please write your explanation to or against my claim. If you are searching for reasons, the keyword 'categorical data' may be useful. Although the letters in our case are not treated as categorical data, the same reasons as for categorical data is applicable. Even if my claim is valid, at the end it won't matter due to something called \"embedding layers\" that we will use in our model. What is an embedding layer? What is it's purpose? Explain.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de3f69-cd06-4adc-8150-2522802f345a",
   "metadata": {
    "deletable": false,
    "id": "Ans2"
   },
   "source": [
    "**Answer** \n",
    "\n",
    "Encoding letters as numbers based on their alphabetical order is a poor approach for feeding data into learning models. This is because such numerical representations impose an artificial ordinal relationship between characters that doesn't actually exist. For example, this encoding suggests that 'b' is closer to 'a' than to 'z', which may mislead the model into learning patterns based on this false ordering.\n",
    "\n",
    "This is similar to problems faced when using numerical labels for categorical data. In such cases, treating categories as numbers introduces arbitrary relationships that can hurt model performance. Letters, like categories, are discrete and non-numeric by nature, so treating them as ordinal values is inappropriate.\n",
    "\n",
    "However, this issue is mitigated by the use of embedding layers in neural networks. An embedding layer maps each input token (in our case, a letter index) to a learned dense vector of fixed size. These vectors capture meaningful relationships between inputs through training and allow the model to learn distributed representations, rather than relying on raw numeric indices.\n",
    "\n",
    "The purpose of an embedding layer is to transform sparse or index-based inputs into a continuous vector space where similar inputs (e.g., characters that often appear in similar contexts) can have similar representations. This makes the model more powerful and capable of generalizing better from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070a74-0f42-435d-a3ba-b38f0d1aaf3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 2: Implementing the Autocomplete model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb965e-afcd-41ae-86f0-2f3d4682e18b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will implement a RNN LSTM model. The [video tutorial](https://www.youtube.com/watch?v=tL5puCeDr-o) will be useful. Our model will be only one hidden layer, but feel free to sophisticate with more layers after the project for your own experiments.\n",
    "\n",
    "Our model will contain all the training and prediction methods as single package in a class (autocompleteModel) we will define and implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0dfe311c-669d-4d58-a833-ae3970b6d271",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4976fc91-2c4e-497a-954e-9014dd31be5e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "class autocompleteModel(nn.Module):\n",
    "\n",
    "    #Constructor\n",
    "    def __init__(self, alphabet_size, embed_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        #Set the input parameters to self parameters\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Initialize the layers in the model:\n",
    "        #1 embedding layer, 1 - LSTM cell (hidden layer), 1 fully connected layer with linear activation\n",
    "        self.embed_layer = nn.Embedding(num_embeddings=self.alphabet_size, embedding_dim=self.embed_dim)\n",
    "        self.LSTM_cell = nn.LSTMCell(input_size=self.embed_dim, hidden_size=self.hidden_size)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.alphabet_size)\n",
    "\n",
    "    #Feedforward\n",
    "    def forward(self, character, hidden_state, cell_state):\n",
    "        #Perform feedforward in order\n",
    "        #1. Embed the input (one charcter represented by a number)\n",
    "        #2. Feed the embedded output to the LSTM cell\n",
    "        #3. Feed the LSTM output to the fully connected layer to obtain the output\n",
    "        #4. return the output, and both the hidden state and cell state from the LSTM cell output\n",
    "        embedding = self.embed_layer(character)\n",
    "        hidden_state, cell_state = self.LSTM_cell(embedding, (hidden_state, cell_state))\n",
    "        output = self.fc(hidden_state)\n",
    "        \n",
    "        return output, hidden_state, cell_state\n",
    "\n",
    "    #Intialize the first hidden state and cell state (for the start of a word) as zero tensors of required length.\n",
    "    def initial_state(self):\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        c0 = torch.zeros(1, self.hidden_size)\n",
    "        \n",
    "        return (h0, c0)\n",
    "\n",
    "    #Train the model in epochs given the vocab, the training will be fed in batches of batch_size\n",
    "    def trainModel(self, vocab, epochs = 5, batch_size = 100):\n",
    "        #Convert the model into train mode\n",
    "        self.train()\n",
    "\n",
    "        #Set the optimizer (ADAM), you may need to provide the model parameters  and learning rate\n",
    "        optimizer = optim.Adam(self.parameters(), LEARNING_RATE)\n",
    "\n",
    "        #Keep a log of the loss at the end of each training cycle.\n",
    "        loss_log = []\n",
    "\n",
    "        for e in range(epochs):\n",
    "            random.shuffle(vocab)\n",
    "            num_iter = len(vocab) // batch_size\n",
    "\n",
    "            for i in range(num_iter):\n",
    "                # Set the loss to zero, initialize the optimizer with zero_grad at the beginning of each training cycle.\n",
    "                batch_loss = 0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # extract the batch\n",
    "                vocab_batch = vocab[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "                for word in vocab_batch:\n",
    "                    # Initialize the hidden state and cell state at the start of each word.\n",
    "                    hidden_state, cell_state = self.initial_state()\n",
    "\n",
    "                    # Convert the word into a tensor of number and create input and target from the word\n",
    "                    #Input will be the first WORD_SIZE - 1 charcters and target is the last WORD_SIZE - 1 charcters\n",
    "                    input_tensor = word_to_numlist(word[:WORD_SIZE-1])\n",
    "                    target_tensor = word_to_numlist(word[1:WORD_SIZE])\n",
    "\n",
    "                    #Loop through each character (as a number) in the word\n",
    "                    for c in range(WORD_SIZE - 1):\n",
    "                        # Feed the cth character to the model (feedforward) and comput the loss (use cross entropy in torch)\n",
    "                        output, hidden_state, cell_state = self.forward(input_tensor[c].unsqueeze(0), hidden_state, cell_state)\n",
    "                        loss = nn.functional.cross_entropy(output, target_tensor[c].view(1))\n",
    "                        batch_loss += loss\n",
    "                    \n",
    "                # Compute the average loss per word in the batch and perform backpropagation (.backward())\n",
    "                batch_loss /= len(vocab_batch)\n",
    "                batch_loss.backward()\n",
    "                    \n",
    "                #Update model parameters using the optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                #Update the loss_log \n",
    "                loss_log.append(batch_loss.item())\n",
    "\n",
    "            print(\"Epoch: \", e)\n",
    "\n",
    "        # Plot a graph of the variation of the loss.\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(loss_log)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss vs Epochs')\n",
    "        plt.show()\n",
    "\n",
    "    #Perform autocmplete given a sample of strings (typically 3-5 starting letters)\n",
    "    def autocomplete(self, sample):\n",
    "        #Convert the model into evaluation mode\n",
    "        self.eval()\n",
    "        completed_list = []\n",
    "\n",
    "        # In the following loop for each sample item initialize hidden and cell states, then predict the remaining characters\n",
    "        #You will have to convert the output into a softmax (you may use your softmax method from the last project) probability distribution, then use torch.multinomial \n",
    "        for literal in sample:\n",
    "            # Initialize the hidden state and cell state at the start of each word.\n",
    "            hidden_state, cell_state = self.initial_state()\n",
    "\n",
    "            # Convert the word into a tensor of number\n",
    "            input_tensor = word_to_numlist(literal)\n",
    "            predicted = literal\n",
    "\n",
    "            # calculate of hidden state given characters\n",
    "            for p in range(len(literal) - 1):\n",
    "                init_input = input_tensor[p].unsqueeze(0)\n",
    "                _, hidden_state, cell_state = self.forward(init_input, hidden_state, cell_state)\n",
    "            \n",
    "            init_input = input_tensor[-1].unsqueeze(0)\n",
    "            \n",
    "            # generating sequence\n",
    "            for g in range(WORD_SIZE - len(literal)):\n",
    "                # generate\n",
    "                output, hidden_state, cell_state = self.forward(init_input, hidden_state, cell_state)\n",
    "                output_prob = nn.functional.softmax(output, dim = 1)\n",
    "                top_1 = torch.multinomial(output_prob, 1)[0]\n",
    "                \n",
    "                # prediction\n",
    "                pred_char = num_to_char(top_1)\n",
    "                predicted += pred_char\n",
    "                init_input = torch.tensor(char_to_num(pred_char)).unsqueeze(0)\n",
    "\n",
    "            completed_list.append(predicted)\n",
    "            \n",
    "        return completed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b5489-b770-4519-b20c-4f2beebfb8f9",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Section 3: Using and evaluating the model\n",
    "\n",
    "(i) Initialize and train autocompleteModels using different embedding dimensions and hidden layer sizes. Use different learning rates, epochs, batch sizes. Train the best model you can.\n",
    "\n",
    "(ii) Evaluate it on different samples of partially filled in words to test your model. Eg: [\"univ\", \"math\", \"neur\", \"engin\"] etc.\n",
    "\n",
    "(iii) Set your best model, to the variable best_model. This model will be tested against random inputs (3-4 starting strings of common English words). **This will be the main contributor for your score in this project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ac2ab12-0cc8-48d1-816f-ab8ec7ac23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c11b1ecf-456c-408c-bb06-6f3ab91234f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best_model_weights.pth', <http.client.HTTPMessage at 0x290d4ade5d0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/isharadilshanra/Deeplearning/main/RNN/best_model_weights.pth\"\n",
    "save_path = \"best_model_weights.pth\"\n",
    "\n",
    "urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0266186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\KMSpico\\temp\\ipykernel_20272\\2835593788.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "autocompleteModel(\n",
       "  (embed_layer): Embedding(27, 64)\n",
       "  (LSTM_cell): LSTMCell(64, 256)\n",
       "  (fc): Linear(in_features=256, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_size = 27\n",
    "embed_dim = 64\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "model = autocompleteModel(alphabet_size, embed_dim, hidden_size, num_layers)\n",
    "model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a60fabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_autocomplete_model(model, word_segments):\n",
    "    predictions = model.autocomplete(word_segments)\n",
    "    \n",
    "    for segment, prediction in zip(word_segments, predictions):\n",
    "        processed_prediction = prediction.replace(\"_\", \"\")\n",
    "        print(f'word segment : {segment} || predicted word : {processed_prediction}')\n",
    "    \n",
    "    print(predictions)\n",
    "\n",
    "    # Save the model as best model (can later be modified to track accuracy too)\n",
    "    best_model = model\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ba00173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word segment : comp || predicted word : comprece\n",
      "word segment : phys || predicted word : physic\n",
      "word segment : stat || predicted word : statul\n",
      "word segment : bio || predicted word : bios\n",
      "word segment : therm || predicted word : thermal\n",
      "word segment : mech || predicted word : mechanism\n",
      "word segment : electr || predicted word : electricity\n",
      "word segment : astro || predicted word : astronomy\n",
      "word segment : chem || predicted word : chem\n",
      "['comprece_____', 'physic_______', 'statul_______', 'bios_________', 'thermal______', 'mechanism____', 'electricity__', 'astronomy____', 'chem_________']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_seg = [\"comp\", \"phys\", \"stat\", \"bio\", \"therm\", \"mech\", \"electr\", \"astro\", \"chem\"]\n",
    "\n",
    "best_model = evaluate_autocomplete_model(model, word_seg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
